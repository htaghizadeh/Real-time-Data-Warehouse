{
  "metadata" : {
    "config" : {
      "dependencies" : {
        "scala" : [
          "io.delta:delta-core_2.12:1.0.0",
          "org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2",
          "org.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.2"
        ]
      },
      "exclusions" : [
      ],
      "repositories" : [
      ],
      "sparkConfig" : {
        "spark.sql.extensions" : "io.delta.sql.DeltaSparkSessionExtension",
        "spark.sql.catalog.spark_catalog" : "org.apache.spark.sql.delta.catalog.DeltaCatalog"
      },
      "env" : {
        
      },
      "scalaVersion" : "2.12"
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# <br>\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1629964050982,
          "endTs" : 1629964051650
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "spark.version"
      ],
      "outputs" : [
        {
          "execution_count" : 1,
          "data" : {
            "text/plain" : [
              "3.1.2"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "String"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1629964051670,
          "endTs" : 1629964051917
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import org.apache.commons.io.FileUtils\n",
        "import org.apache.spark.sql.{DataFrame, Encoder, SparkSession}\n",
        "import org.apache.spark.sql.functions.{col, explode, lit}\n",
        "import org.apache.spark.sql.streaming.{StreamingQuery, Trigger}\n",
        "import org.apache.spark.sql.types.StructType"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1629964051921,
          "endTs" : 1629964052124
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import spark.implicits._\n",
        "import org.apache.spark.sql._"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1629964052129,
          "endTs" : 1629964053207
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import java.sql.Timestamp\n",
        "\n",
        "\n",
        "case class DebeziumEvent(payload: Payload)\n",
        "case class Payload(op: String, source: Source, before: String, after: String)\n",
        "case class Source(ts_ms: Long, lsn: Long, table: String)\n",
        "case class Member(id: Int,\n",
        "                    c_d_id: Float,\n",
        "                    first_name: String,\n",
        "                    last_name: String,\n",
        "                    address: String,\n",
        "                    address_city: String,\n",
        "                    address_country: String,\n",
        "                    insurance_company: String,\n",
        "                    insurance_number: String,\n",
        "                    ts_created: String,\n",
        "                    ts_updated: String)\n",
        "\n",
        "case class AccidentClaims(claim_id: Int,\n",
        "                  claim_total: Float,\n",
        "                  claim_total_receipt: String,\n",
        "                  claim_currency: String,\n",
        "                  member_id: Int,\n",
        "                  accident_date: String,\n",
        "                  accident_type: String,\n",
        "                  accident_detail: String,\n",
        "                  claim_date: String,\n",
        "                  claim_status: String,\n",
        "                  ts_created: String,\n",
        "                  ts_updated: String\n",
        "                  )"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 5,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1629964053214,
          "endTs" : 1629964054674
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val inputSchema = implicitly[Encoder[DebeziumEvent]].schema"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 6,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1629964054683,
          "endTs" : 1629964055419
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val inputDF = spark\n",
        "      .readStream\n",
        "      .format(\"kafka\")\n",
        "      .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
        "      .option(\"subscribe\", \"pg_claims.claims.members\")\n",
        "      .option(\"startingOffsets\", \"earliest\")\n",
        "      .load()"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 7,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1629964055436,
          "endTs" : 1629964055985
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        " import org.apache.spark.sql._\n",
        "\n",
        "\n",
        "val extractedDF = inputDF\n",
        "      .selectExpr(\"CAST(value AS STRING)\")\n",
        "      .select(functions.from_json($\"value\", inputSchema).as(\"data\"))"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 8,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1629964055990,
          "endTs" : 1629964056236
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "def writeAsDelta(deltaTableName: String, df: DataFrame) = {\n",
        "    val query = df\n",
        "      .writeStream\n",
        "      .trigger(Trigger.ProcessingTime(\"5 seconds\"))\n",
        "      .foreachBatch { (batchDF: DataFrame, batchID: Long) =>\n",
        "        println(s\"Writing to Delta $batchID\")\n",
        "        batchDF.write\n",
        "          .format(\"delta\")\n",
        "          .mode(\"append\")\n",
        "          .save(deltaTableName)\n",
        "      }\n",
        "      .outputMode(\"update\")\n",
        "      .start()\n",
        "    query\n",
        "  }"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 9,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1629964056249,
          "endTs" : 1629964056882
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "writeAsDelta(\"/tmp/data/staging/\" + \"members\", extractedDF).awaitTermination()"
      ],
      "outputs" : [
        {
          "ename" : "org.apache.spark.sql.streaming.StreamingQueryException",
          "evalue" : "Query [id = 45d0b91d-c2c1-4931-9e8a-4d09618311e8, runId = 60a88a40-09cf-45f6-a0fc-693a623a5f56] terminated with exception: org/apache/spark/kafka010/KafkaConfigUpdater",
          "traceback" : [
            "org.apache.spark.sql.execution.streaming.StreamExecution,org$apache$spark$sql$execution$streaming$StreamExecution$$runStream,StreamExecution.scala,356",
            "org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1,run,StreamExecution.scala,244"
          ],
          "output_type" : "error"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 10,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1629963055257,
          "endTs" : 1629963055466
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "spark.read.format(\"delta\").load(\"/tmp/data/staging/members\").show()"
      ],
      "outputs" : [
      ]
    }
  ]
}